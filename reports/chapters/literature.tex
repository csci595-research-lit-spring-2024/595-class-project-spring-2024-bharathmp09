\chapter{Literature Review}
\label{ch:lit_rev} %Label of the chapter lit rev. The key ``ch:lit_rev'' can be used with command \ref{ch:lit_rev} to refer this Chapter.


This section explores the existing existing body of knowledge surrounding the Comparative Analysis of Denoising Autoencoder (DAE) and Convolutional Neural Networks (CNN) for MNIST Classification. Recent advancements in deep machine learning have significantly improved feature learning in image processing tasks, particularly in image classification, making it imperative to explore and understand the capabilities and limitations of different models. Notably, Convolutional Neural Networks (CNNs) have emerged as powerful tools, showcasing superior performance in image classification tasks. However, the inherent challenge of noise in real-world data poses a significant constraint on the performance of deep neural networks, prompting the exploration of denoising techniques. The literature review will examine prior studies focusing on image denoising methods, including traditional approaches and those leveraging deep learning, with a specific emphasis on Denoising Autoencoders. Understanding the existing landscape is crucial for contextualizing the current research and identifying gaps that the Comparative Analysis seeks to address.

\section{Review of state-of-the-art}
The \citep{8078730} explores the application of deep learning, specifically Convolutional Neural Networks (CNNs), in image classification. The authors discuss various models commonly used in deep learning, such as Auto Encoder, sparse coding, Restricted Boltzmann Machine, Deep Belief Networks, and CNNs. They emphasize the effectiveness of CNNs in image classification, particularly showcasing their high performance. The study involves building a simple CNN for image classification, with experiments conducted on benchmark datasets like minist and cifar-10. The authors delve into the fundamental components of CNNs, including Convolutional layers, pooling layers, and fully-connected layers. The study focuses on learning rate strategies and optimization algorithms for solving optimal parameters in image classification. Different strategies are compared, demonstrating the impact on recognition rates during training and testing. 
The experimental results and 

The \citep{bajaj2020autoencoders} proposes an efficient image denoising technique using autoencoders based on deep learning models. The motivation behind image denoising is to eliminate noise from images, which can be caused by various factors such as defects in camera sensors, transmission in noisy channels, or faulty memory locations in hardware. The proposed model utilizes autoencoders, a type of artificial neural network, for image denoising. Autoencoders learn noise patterns from training images and attempt to eliminate noise from novel images. The proposed network architecture consists of convolutional denoising autoencoder (CDA) blocks, each containing internal layers like convolution, pooling, deconvolution, and upsampling. Skip connections are employed to enhance the model's ability to capture finer image details and facilitate backpropagation during training. The paper discusses related work, highlighting methods such as non-local means, Block Matching and 3D Matching (BM3D), and denoising autoencoders. STL-10 dataset for training and the SET5 standard image dataset for testing. Performance metrics, such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM), are used to evaluate the proposed model's effectiveness in comparison to baseline models. The results show that the proposed model outperforms Convolutional Denoising Deep Neural Network (CDDNN) and Residual Encoder Decoder with 30 layers (RED30) in terms of PSNR, indicating superior denoising capabilities.

\section{Convolutional Neural Networks}
CNNs are a variant of multi-layered neural networks designed for
processing two-dimensional data, particularly well-suited for classifying visual patterns like pixel images with minimal pre-processing.
Feature extraction from observed data occurs at each layer by 
applying digital filtering techniques, allowing information to propagate through multiple layers.

THe CNN architecture, introduced by LeCun et al. in 1998, are
applied to classify images and perceive visual patterns directly
from pixel images. Information propagation throughout multiple layers
enables feature extraction using digital filtering techniques.
CNNs perform two main processes: convolution and subsampling. 
The convolution involves applying a small-sized kernel over the input
feature map (IFM) to produce a convolved feature map (CFM). CFMs are
computed by applying the convolutional operation over the original
input image using a kernel, which represents weights and biases.
Weights are shared among positions, preserving spatial locality during
the convolution process. The subsampling simplifies the feature map 
gained from convolution by selecting significant features and 
discarding the rest. Max-pooling, a subsampling method used in 
experiments, involves taking the maximum value over non-overlapping
sub-regions.

\section{Denoising Auto-Encoder}
Denoising Autoencoder (DAE) is a specialized type of artificial neural network designed to clean up noisy or corrupted data. It belongs to the family of autoencoders, which are neural networks trained to learn efficient representations of input data. DAEs, in particular, excel in removing noise and capturing essential features from corrupted input.

The encoder is the first part of the DAE. Its role is to transform the input data into a compressed representation. This compressed representation should ideally retain the essential features of the input while filtering out noise.
One distinctive feature of DAE is the intentional introduction of noise into the input data. This could involve adding random variations or distortions to simulate real-world noise or corruption.The decoder is the second part of the DAE. It takes the compressed representation from the encoder and reconstructs the clean version of the original input data. The decoder's task is to recover the essential features and remove the injected noise.

% A possible section of you chapter
\section{Critique of the review} % Use this section title or choose a betterone
The literature review provides a comprehensive overview of the existing body of knowledge on the Comparative Analysis of Denoising Autoencoder (DAE) and Convolutional Neural Networks (CNN) for MNIST Classification. The review is structured logically, beginning with an introduction to the significance of the topic and progressing to a review of state-of-the-art research.

\begin{itemize}
    \item Clear Problem Statement: The literature review effectively establishes the problem by highlighting the challenges posed by noise in real-world data for deep neural networks. The motivation to explore denoising techniques is well justified in the context of advancements in deep machine learning and the superior performance of CNNs in image classification.

    \item Thorough State-of-the-Art Review: The review of state-of-the-art research is well-detailed, discussing relevant models in deep learning for image classification, such as Auto Encoder, sparse coding, Restricted Boltzmann Machine, Deep Belief Networks, and CNNs. The exploration of fundamental components of CNNs, learning rate strategies, and optimization algorithms adds depth to the understanding of the field.

    \item Comprehensive Denoising Technique Overview: The literature appropriately includes a discussion on denoising techniques, with a specific emphasis on the proposed model using autoencoders. The incorporation of related work, including traditional methods like non-local means and modern approaches like denoising autoencoders, enriches the background information.

    \item Methodology Explanation: The detailed description of the proposed image denoising technique using convolutional denoising autoencoder (CDA) blocks, skip connections, and performance metrics (PSNR and SSIM) adds transparency to the methodology.

    \item Effective Comparison of Models: The comparative analysis between the proposed model and existing models like Convolutional Denoising Deep Neural Network (CDDNN) and Residual Encoder Decoder with 30 layers (RED30) is valuable. The use of performance metrics provides a quantitative basis for comparison.

% Pleae use this section
\section{Summary} 

In this literature review, the Comparative Analysis of Denoising Autoencoder (DAE) and Convolutional Neural Networks (CNN) for MNIST Classification is explored within the context of recent advancements in deep machine learning. Convolutional Neural Networks (CNNs) have shown superior performance in image classification tasks, but the challenge of noise in real-world data necessitates the investigation of denoising techniques. The review critically examines prior studies on image denoising, with a specific focus on Denoising Autoencoders.

The state-of-the-art review discusses the application of CNNs in image classification, emphasizing their effectiveness. It covers various deep learning models, including Auto Encoder, sparse coding, Restricted Boltzmann Machine, and Deep Belief Networks, with experiments conducted on benchmark datasets. Another study proposes an image denoising technique using autoencoders, demonstrating its effectiveness through metrics like Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). The proposed model outperforms existing models, showcasing superior denoising capabilities. The comprehensive overview of denoising techniques, methodology transparency, and effective comparison of models contribute to the overall strength of the literature review. This groundwork sets the stage for the Comparative Analysis, aiming to address gaps in understanding and provide insights into the capabilities and limitations of DAEs and CNNs for MNIST Classification.